{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SymbolAlreadyExposedError",
     "evalue": "Symbol pad_sequences is already exposed as ('keras.preprocessing.sequence.pad_sequences',).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mSymbolAlreadyExposedError\u001B[0m                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-33-e51c3225ca86>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnp_utils\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mGlobalMaxPooling1D\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mConv1D\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mMaxPooling1D\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mFlatten\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBidirectional\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSpatialDropout1D\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpreprocessing\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msequence\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mEarlyStopping\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/preprocessing/sequence.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m \u001B[0mkeras_export\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'keras.preprocessing.sequence.pad_sequences'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpad_sequences\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     93\u001B[0m keras_export(\n\u001B[1;32m     94\u001B[0m     'keras.preprocessing.sequence.make_sampling_table')(make_sampling_table)\n",
      "\u001B[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/tf_export.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, func)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    333\u001B[0m     \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mundecorated_func\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_decorator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munwrap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 334\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_attr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mundecorated_func\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mapi_names_attr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_names\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    335\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_attr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mundecorated_func\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mapi_names_attr_v1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_names_v1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    336\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/tf_export.py\u001B[0m in \u001B[0;36mset_attr\u001B[0;34m(self, func, api_names_attr, names)\u001B[0m\n\u001B[1;32m    344\u001B[0m         raise SymbolAlreadyExposedError(\n\u001B[1;32m    345\u001B[0m             \u001B[0;34m'Symbol %s is already exposed as %s.'\u001B[0m \u001B[0;34m%\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 346\u001B[0;31m             (func.__name__, getattr(func, api_names_attr)))  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m    347\u001B[0m     \u001B[0msetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mapi_names_attr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    348\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mSymbolAlreadyExposedError\u001B[0m: Symbol pad_sequences is already exposed as ('keras.preprocessing.sequence.pad_sequences',)."
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "#                               IMPORTING LIBRARIES\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold,KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, model_selection, pipeline\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "from tensorflow_core.python.keras.models import Sequential\n",
    "from tensorflow_core.python.keras.layers.recurrent import LSTM, GRU\n",
    "from tensorflow_core.python.keras.layers.core import Dense, Activation, Dropout\n",
    "from tensorflow_core.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow_core.python.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow_core.python.keras.utils import np_utils\n",
    "from tensorflow_core.python.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from tensorflow_core.python.keras.preprocessing import sequence, text\n",
    "from tensorflow_core.python.keras.callbacks import EarlyStopping\n",
    "\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                               FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "# ----------------------\n",
    "# CREATING INITIAL DF\n",
    "# ----------------------\n",
    "df = pd.read_csv(\"./input/fake_job_postings.csv\")\n",
    "df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# EXTRACTING TEXT FEATURES\n",
    "# --------------------------\n",
    "text_df = df[['title', 'company_profile', 'description', 'requirements',\n",
    "              'benefits', 'fraudulent']]\n",
    "text_df = text_df.fillna(' ')\n",
    "text_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# EXTRACTING CATEGORICAL FEATURES\n",
    "# ------------------------------------\n",
    "cat_df = df[['telecommuting', 'has_company_logo', 'has_questions',\n",
    "             'employment_type', 'required_experience', 'required_education',\n",
    "             'industry', 'function', 'fraudulent']]\n",
    "cat_df = cat_df.fillna('None')\n",
    "cat_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# INITIALIZING TARGET\n",
    "# -----------------------\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(17, 5), dpi=(100))\n",
    "plt.tight_layout()\n",
    "\n",
    "df['fraudulent'].value_counts().plot(kind='pie', ax=axes[0],\n",
    "                                     labels=['Real Post (95%)', 'Fake Post (5%)'])\n",
    "temp = df[\"fraudulent\"].value_counts()\n",
    "sns.barplot(temp.index, temp, ax=axes[1])\n",
    "\n",
    "axes[0].set_ylabel(' ')\n",
    "axes[1].set_ylabel(' ')\n",
    "axes[1].set_xticklabels(['Real Post (17014)', 'Fake Post (866)'])\n",
    "\n",
    "axes[0].set_title('Target Distribution in Dataset', fontsize=13)\n",
    "axes[1].set_title('Target Count in Dataset', fontsize=13)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                      EXPLORATORY ANALYSIS OF JOB POSTINGS\n",
    "# ============================================================================\n",
    "cat_cols = ['telecommuting', 'has_company_logo', 'has_questions',\n",
    "            'employment_type', 'required_experience', 'required_education']\n",
    "import matplotlib.gridspec as gridspec\n",
    "grid = gridspec.GridSpec(3, 3, wspace=0.5, hspace=0.5)\n",
    "plt.figure(figsize=(15, 25))\n",
    "\n",
    "for n, col in enumerate(cat_df[cat_cols]):\n",
    "    ax = plt.subplot(grid[n]) # feeding the figure of grid\n",
    "    sns.countplot(x=col, data=cat_df, hue='fraudulent', palette='Set2')\n",
    "    ax.set_ylabel('Count', fontsize=12) # y axis label\n",
    "    ax.set_title(f'{col} Distribution by Target', fontsize=15) # title label\n",
    "    ax.set_xlabel(f'{col} values', fontsize=12) # x axis label\n",
    "    xlabels = ax.get_xticklabels()\n",
    "    ylabels = ax.get_yticklabels()\n",
    "    ax.set_xticklabels(xlabels,  fontsize=10)\n",
    "    ax.set_yticklabels(ylabels,  fontsize=10)\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.xticks(rotation=90)\n",
    "    total = len(cat_df)\n",
    "    sizes=[] # Get highest values in y\n",
    "    for p in ax.patches: # loop to all objects\n",
    "        height = p.get_height()\n",
    "        sizes.append(height)\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(height/total*100),\n",
    "                ha=\"center\", fontsize=10)\n",
    "    ax.set_ylim(0, max(sizes) * 1.15)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# EVALUATING NUMBER OF CHARACTERS IN COMPANY PROFILE\n",
    "# ------------------------------------------------------------\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\n",
    "length=text_df[text_df['fraudulent']==1]['description'].str.len()\n",
    "ax1.hist(length, bins=20, color='orangered')\n",
    "ax1.set_title('Fake Posts')\n",
    "length=text_df[text_df['fraudulent']==0]['description'].str.len()\n",
    "ax2.hist(length, bins=20)\n",
    "ax2.set_title('Real Posts')\n",
    "fig.suptitle('Characters in Company Description')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# EVALUATING NUMBER OF CHARACTERS IN JOB REQUIREMENTS\n",
    "# --------------------------------------------------------\n",
    "fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\n",
    "length=text_df[text_df[\"fraudulent\"]==1]['requirements'].str.len()\n",
    "ax1.hist(length,bins = 20,color='orangered')\n",
    "ax1.set_title('Fake Posts')\n",
    "length=text_df[text_df[\"fraudulent\"]==0]['requirements'].str.len()\n",
    "ax2.hist(length,bins = 20)\n",
    "ax2.set_title('Real Posts')\n",
    "fig.suptitle('Characters in requirements')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# EVALUATING NUMBER OF CHARACTERS IN JOB BENEFITS\n",
    "# --------------------------------------------------------\n",
    "fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\n",
    "length=text_df[text_df[\"fraudulent\"]==1]['benefits'].str.len()\n",
    "ax1.hist(length,bins = 20,color='orangered')\n",
    "ax1.set_title('Fake Post')\n",
    "length=text_df[text_df[\"fraudulent\"]==0]['benefits'].str.len()\n",
    "ax2.hist(length,bins = 20)\n",
    "ax2.set_title('Real Post')\n",
    "fig.suptitle('Characters in benefits')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                               DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "text = text_df[text_df.columns[0:-1]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)\n",
    "target = df['fraudulent']\n",
    "\n",
    "print(len(text))\n",
    "print(len(target))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# ANALYSIS OF UNIGRAMS AND BIGRAMS\n",
    "# --------------------------------------------------------\n",
    "def get_top_tweet_unigrams(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def get_top_tweet_bigrams(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(18, 30), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "top_unigrams=get_top_tweet_unigrams(text)[:50]\n",
    "x,y=map(list,zip(*top_unigrams))\n",
    "sns.barplot(x=y,y=x, ax=axes[0], color='teal')\n",
    "\n",
    "\n",
    "top_bigrams=get_top_tweet_bigrams(text)[:50]\n",
    "x,y=map(list,zip(*top_bigrams))\n",
    "sns.barplot(x=y,y=x, ax=axes[1], color='crimson')\n",
    "\n",
    "\n",
    "axes[0].set_ylabel(' ')\n",
    "axes[1].set_ylabel(' ')\n",
    "\n",
    "axes[0].set_title('Top 50 most common unigrams in text', fontsize=15)\n",
    "axes[1].set_title('Top 50 most common bigrams in text', fontsize=15)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# TEXT CLEANING\n",
    "# ---------------------------\n",
    "def clean_text(text):\n",
    "    '''MAKE TEXT LOWERCASE, REMOVE TEXT IN SQUARE BRACKETS,REMOVE LINKS,REMOVE PUNCTUATION\n",
    "    AND REMOVE WORDS CONTAINING NUMBERS.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "text = text.apply(lambda x: clean_text(x))\n",
    "text.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CREATING TOKENIZER\n",
    "# --------------------------\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# APPLYING TOKENIZER\n",
    "text = text.apply(lambda x: tokenizer.tokenize(x))\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stop_words]\n",
    "    return words\n",
    "text = text.apply(lambda x : remove_stopwords(x))\n",
    "\n",
    "def combine_text(list_of_text):\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "text = text.apply(lambda x: combine_text(x))\n",
    "text.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                             BASELINE MODEL\n",
    "# ============================================================================\n",
    "# ------------------\n",
    "# INITIALIZE MODEL\n",
    "# ------------------\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "auc_buf = []\n",
    "count = 0\n",
    "predictions = 0\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# ENUMERATES THE SPLITS AND SUMMARIZES THE PREDICTIONS\n",
    "# ------------------------------------------------------\n",
    "for train_ix, test_ix in kfold.split(text, target):\n",
    "    print('Fold {}'.format(count + 1))\n",
    "    train_X, test_X = text[train_ix], text[test_ix]\n",
    "    train_y, test_y = target[train_ix], target[test_ix]\n",
    "\n",
    "    # Appling Count Vectorizer\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    train_X_vec = count_vectorizer.fit_transform(train_X)\n",
    "    test_X_vec = count_vectorizer.transform(test_X)\n",
    "\n",
    "    lr = LogisticRegression(C=0.1, solver='lbfgs', max_iter=1000, verbose=0, n_jobs=-1)\n",
    "    lr.fit(train_X_vec, train_y)\n",
    "    preds = lr.predict(test_X_vec)\n",
    "\n",
    "    auc = roc_auc_score(test_y, preds)\n",
    "    print('{} AUC: {}'.format(count, auc))\n",
    "    auc_buf.append(auc)\n",
    "    count += 1\n",
    "\n",
    "print('AUC mean score = {:.6f}'.format(np.mean(auc_buf)))\n",
    "print('AUC std score = {:.6f}'.format(np.std(auc_buf)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# SPLITTING THE DATA FOR THE GLOVE\n",
    "# -------------------------------------\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(text, target, test_size=0.2, random_state=4, stratify=target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# LOADS THE GLOVE VECTORS INTO A DICTIONARY\n",
    "# --------------------------------------------\n",
    "embeddings_index={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embeddings_index[word]=vectors\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# CREATES A NORMALIZED VECTOR FOR THE WHOLE SENTENCE\n",
    "# ----------------------------------------------------\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(200)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CREATES GLOVE FEATURES\n",
    "# --------------------------\n",
    "xtrain_glove = np.array([sent2vec(x) for x in tqdm(X_train)])\n",
    "xtest_glove = np.array([sent2vec(x) for x in tqdm(X_test)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "#                          DEEP LEARNING TIME BABY\n",
    "# ============================================================================\n",
    "\n",
    "# ----------------------------------------------\n",
    "# SCALING THE DATA BEFORE CREATING NEURAL NET\n",
    "# ----------------------------------------------\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xtest_glove_scl = scl.transform(xtest_glove)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# CREATING A 2 LAYER SEQUENTIAL NEURAL NET\n",
    "# -------------------------------------------\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(200, input_dim=200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# COMPILE THE MODEL\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# FITTING MODEL\n",
    "# ---------------\n",
    "model.fit(xtrain_glove_scl, y=y_train, batch_size=64,\n",
    "          epochs=10, verbose=1,\n",
    "          validation_data=(xtest_glove_scl, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# MAKING PREDICTIONS\n",
    "# -----------------------\n",
    "predictions = model.predict(xtest_glove_scl)\n",
    "predictions = np.round(predictions).astype(int)\n",
    "print('2 layer sequential neural net on GloVe Feature')\n",
    "print (\"AUC score :\", np.round(roc_auc_score(y_test, predictions),5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# SIMPLE BERT IMPLEMENTATION\n",
    "# -----------------------------\n",
    "new_text = text_df[text_df.columns[0:-1]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)\n",
    "target = df['fraudulent']\n",
    "\n",
    "def clean_text(text):\n",
    "    '''MAKE TEXT LOWERCASE, REMOVE TEXT IN SQUARE BRACKETS,REMOVE LINKS,REMOVE PUNCTUATION\n",
    "    AND REMOVE WORDS CONTAINING NUMBERS.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# APPLYING THE CLEANING FUNCTION TO BOTH TEST AND TRAINING DATASETS\n",
    "new_text = new_text.apply(lambda x: clean_text(x))\n",
    "new_text.head(3)\n",
    "\n",
    "# TRYING FIRST 2000 SAMPLE\n",
    "batch_1 = new_text[:2000]\n",
    "target_1 = target[:2000]\n",
    "target_1.value_counts()\n",
    "\n",
    "# DISTILBERT\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "# LOAD PRETRAINED MODEL/TOKENIZER\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# TOKENIZATION\n",
    "# ----------------\n",
    "tokenized = batch_1.apply((lambda x: tokenizer.encode(x, max_length = 60, add_special_tokens=True)))\n",
    "tokenized.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# PADDING ==> CONVERT 1D ARRAY TO 2D ARRAY\n",
    "# -------------------------------------------\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "np.array(padded).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# MASKING ==>  IGNORE (MASK) THE PADDING WE'VE ADDED\n",
    "# -----------------------------------------------------\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# DEEP LEARNING\n",
    "# ----------------\n",
    "input_ids = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "last_hidden_states[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# MODELING\n",
    "# ---------------\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "labels = target_1\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = \\\n",
    "    train_test_split(features, labels)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "predictions = lr_clf.predict(test_features)\n",
    "predictions = np.round(predictions).astype(int)\n",
    "print (\"AUC score :\", np.round(roc_auc_score(test_labels, predictions),5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}